<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico?v=7.4.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.0" color="#222">
  <link rel="alternate" href="/atom.xml" title="L&H SITE" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css?v=7.4.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.4.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: true,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    },
    sidebarPadding: 40
  };
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=IntersectionObserver"></script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-3852401481783193",
    enable_page_level_ads: true
  });
</script>

  <meta name="description" content="初学机器学习，往往容易淹没在浩瀚的属于中，本文归纳总结一下机器学习相关的术语，帮您更好理解神经网络本文大部分翻译自wildml.comAActivation Function（激活函数）使用非线性函数对训练模型中的输出（当然不限于最终输出）进行非线性化处理，这样神经网络可以学习到复杂的决策边界。常用的激活函数包括  sigmoid, tanh, ReLU (Rectified Linear Uni">
<meta name="keywords" content="L&amp;H, L&amp;H Site, Site, L2H, L2H Site, Linux, Wordpress, Interrupt">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习术语归纳">
<meta property="og:url" content="http://l2h.site/2019/07/15/machine-learning-terms/index.html">
<meta property="og:site_name" content="L&amp;H SITE">
<meta property="og:description" content="初学机器学习，往往容易淹没在浩瀚的属于中，本文归纳总结一下机器学习相关的术语，帮您更好理解神经网络本文大部分翻译自wildml.comAActivation Function（激活函数）使用非线性函数对训练模型中的输出（当然不限于最终输出）进行非线性化处理，这样神经网络可以学习到复杂的决策边界。常用的激活函数包括  sigmoid, tanh, ReLU (Rectified Linear Uni">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2020-08-23T05:00:44.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习术语归纳">
<meta name="twitter:description" content="初学机器学习，往往容易淹没在浩瀚的属于中，本文归纳总结一下机器学习相关的术语，帮您更好理解神经网络本文大部分翻译自wildml.comAActivation Function（激活函数）使用非线性函数对训练模型中的输出（当然不限于最终输出）进行非线性化处理，这样神经网络可以学习到复杂的决策边界。常用的激活函数包括  sigmoid, tanh, ReLU (Rectified Linear Uni">
  <link rel="canonical" href="http://l2h.site/2019/07/15/machine-learning-terms/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>机器学习术语归纳 | L&H SITE</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d8b880a7a50fe4f306c58655c0c8db8a";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">L&H SITE</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">两个背包旅行者的网络自留地。分享旅行日记，Linux技术，机器学习，建站技巧</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-works">
      
    

    <a href="/works/" rel="section"><i class="fa fa-fw fa-th"></i>works</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-sitemap">
      
    

    <a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>Sitemap</a>

  </li>
  </ul>

    

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/lambertdev" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="http://l2h.site/2019/07/15/machine-learning-terms/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lambert">
      <meta itemprop="description" content="旅行日记，Linux技术，机器学习，建站技巧">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="L&H SITE">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">机器学习术语归纳

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-07-15 07:30:50" itemprop="dateCreated datePublished" datetime="2019-07-15T07:30:50+00:00">2019-07-15</time>
            </span>
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Changyan: </span>
    
    
      <a title="changyan" href="/2019/07/15/machine-learning-terms/#SOHUCS" itemprop="discussionUrl"><span id="changyan_count_unit" class="post-comments-count hc-comment-count" data-xid="2019/07/15/machine-learning-terms/" itemprop="commentCount"></span></a>
    
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>初学机器学习，往往容易淹没在浩瀚的属于中，本文归纳总结一下机器学习相关的术语，帮您更好理解神经网络</p><p>本文大部分翻译自<a href="http://www.wildml.com/deep-learning-glossary/" target="_blank" rel="noopener">wildml.com</a></p><h3 id="A"><a href="#A" class="headerlink" title="A"></a>A</h3><h4 id="Activation-Function（激活函数）"><a href="#Activation-Function（激活函数）" class="headerlink" title="Activation Function（激活函数）"></a>Activation Function（激活函数）</h4><p>使用非线性函数对训练模型中的输出（当然不限于最终输出）进行非线性化处理，这样神经网络可以学习到复杂的决策边界。常用的激活函数包括  <a href="https://en.wikipedia.org/wiki/Sigmoid_function" target="_blank" rel="noopener">sigmoid</a>, <a href="http://mathworld.wolfram.com/HyperbolicTangent.html" target="_blank" rel="noopener">tanh</a>, <a href="http://www.wildml.com/deep-learning-glossary/#relu" target="_blank" rel="noopener">ReLU (Rectified Linear Unit)</a>以及众多的变种.</p><a id="more"></a>


<h4 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h4><p>一直基于梯度下降的学习算法，可以自适应调整参数的学习速率。作为<br> <a href="http://www.wildml.com/deep-learning-glossary/#adagrad" target="_blank" rel="noopener">Adagrad</a> 的变种，对超参数敏感，容易造成学习速率过快下降。可以作为标准<br> <a href="http://www.wildml.com/deep-learning-glossary/#sgd" target="_blank" rel="noopener">SGD</a>替代。相关文献：</p>
<ul>
<li><p><a href="http://arxiv.org/abs/1212.5701" target="_blank" rel="noopener">ADADELTA: An Adaptive Learning Rate Method</a></p>
</li>
<li><p><a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="noopener">Stanford CS231n: Optimization Algorithms</a>  </p>
</li>
<li><p><a href="http://sebastianruder.com/optimizing-gradient-descent/" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a></p>
</li>
</ul>
<h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><p>Adagrad是一种自适应调整学习速率的算法，它会追踪梯度平方的变化，并对学习速率做自适应调整。对稀疏数据处理非常有效（会对不常更新的参数加快学习速率）。</p>
<ul>
<li><p><a href="http://www.magicbroom.info/Papers/DuchiHaSi10.pdf" target="_blank" rel="noopener">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a>  </p>
</li>
<li><p><a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="noopener">Stanford CS231n: Optimization Algorithms</a>  </p>
</li>
<li><p><a href="http://sebastianruder.com/optimizing-gradient-descent/" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a></p>
</li>
</ul>
<h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><p>类似于 <a href="http://www.wildml.com/deep-learning-glossary/#rmsprop" target="_blank" rel="noopener">rmsprop</a> 的学习速率更新算法，更新主要采取即时的第一和第二时刻平均值。另外算法也包括了bias纠正单元，相关文献：</p>
<ul>
<li><a href="http://arxiv.org/abs/1412.6980" target="_blank" rel="noopener">Adam: A Method for Stochastic Optimization</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a></li>
</ul>
<h4 id="Affine-Layer"><a href="#Affine-Layer" class="headerlink" title="Affine Layer"></a>Affine Layer</h4><p>神经网络的一种全连接层。Affine的含义是：每个上层的神经元链接当前层的神经元，即这是标准的神经网络层。Affine层通常会与 <a href="http://www.wildml.com/deep-learning-glossary/#cnn" target="_blank" rel="noopener">Convolutional Neural Networks</a> 或者 <a href="http://www.wildml.com/deep-learning-glossary/#rnn" target="_blank" rel="noopener">Recurrent Neural Networks</a>  一起使用，用于最终产生一个决策。函数形式通常是$$y=f(Wx+b)$$。W,X,b分别是权值，输入和偏移向量。f通常为非线性函数</p>
<h4 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h4><p>Attention Mechanisms（注意力机制）灵感的源于人类视觉注意力特点 ：可以关注图片上的特定某个区域。注意力机制可以与自然语言处理或者图片识别结构一起工作，帮助神经网络学习到进行决策时该“注意”到哪些部分。相关文献：</p>
<ul>
<li><a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/" target="_blank" rel="noopener">Attention and Memory in Deep Learning and NLP</a></li>
</ul>
<h4 id="Alexnet"><a href="#Alexnet" class="headerlink" title="Alexnet"></a>Alexnet</h4><p>Alexnet是大优势赢得2012年ILSVRC竞赛使用的CNN架构，它使大家重新对使用_<strong>CNN网络</strong>_识别图片的产生兴趣。它由5层卷积层组成，部分卷积层后跟随池化层，最后的全连接层是1000路的softmax分类。Alexnet的介绍见<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks</a>.</p>
<h4 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h4><p>Autoencoder是一种神经网络模型，目标为通过网络中的一些“瓶颈”来预测网络的输入。通过引入瓶颈，强制网络学习到输入的低维度映射，从而有效地压缩输入维度。Autoencoders与PCA即一些其他降维技术有关，因为其本质上的非线性化特点，可以处理更复杂的映射。现有大量的autoencoder架构，包括<a href="http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf" target="_blank" rel="noopener">Denoising Autoencoders</a>,、<a href="http://arxiv.org/abs/1312.6114" target="_blank" rel="noopener">Variational Autoencoders</a>,或者<a href="http://arxiv.org/abs/1511.01432" target="_blank" rel="noopener">Sequence Autoencoders</a></p>
<h4 id="Average-Pooling"><a href="#Average-Pooling" class="headerlink" title="Average-Pooling"></a>Average-Pooling</h4><p>Average-Pooling是<strong><em>卷积神经网络</em></strong>识别图片采用的一种池化技术。工作原理为使用小于图片的窗口在图片特征上进行滑动，取得滑动位置上数值的平均值。从而降低数据特征的维度，同时有效保持数据的特征。与之类似的有最大值池化等方法。</p>
<h3 id="B"><a href="#B" class="headerlink" title="B"></a>B</h3><h4 id="Backpropagation（逆传播，后向传播）"><a href="#Backpropagation（逆传播，后向传播）" class="headerlink" title="Backpropagation（逆传播，后向传播）"></a>Backpropagation（逆传播，后向传播）</h4><p>Backpropagation是有效计算神经网络梯度的方法。它通过微分运算，有效地将误差从输出位置传递到输入位置。它与上世纪70年代开始被使用。文献：</p>
<ul>
<li><a href="http://colah.github.io/posts/2015-08-Backprop/" target="_blank" rel="noopener">Calculus on Computational Graphs: Backpropagation</a></li>
</ul>
<h4 id="Backpropagation-Through-Time-BPTT"><a href="#Backpropagation-Through-Time-BPTT" class="headerlink" title="Backpropagation Through Time (BPTT)"></a>Backpropagation Through Time (BPTT)</h4><p>Backpropagation Through Time (<a href="http://axon.cs.byu.edu/~martinez/classes/678/Papers/Werbos_BPTT.pdf" target="_blank" rel="noopener">paper</a>)是<strong><em>循环神经网络</em></strong>使用的逆传播算法。RNN的网络结构与传统的网络结构不同（每个阶段的神经单元共享参数），因此采用的逆传播也稍后差异。相关介绍见</p>
<ul>
<li><a href="http://axon.cs.byu.edu/~martinez/classes/678/Papers/Werbos_BPTT.pdf" target="_blank" rel="noopener">Backpropagation Through Time: What It Does and How to Do It</a></li>
</ul>
<h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><p>Batch Normalization是对神经网络层输入数据进行小批量分组使用的技术。使用小批量数据分组而非完整数据包可以加速训练速度。其在<strong><em>卷积神经网络</em></strong>或者_<strong>前向神经网络</strong>_使用中被证明非常有效，不过其目前在<strong>循环神经网络</strong>的使用中，效果有限</p>
<ul>
<li><a href="http://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
<li><a href="http://arxiv.org/abs/1510.01378" target="_blank" rel="noopener">Batch Normalized Recurrent Neural Networks</a></li>
</ul>
<h4 id="Bidirectional-RNN"><a href="#Bidirectional-RNN" class="headerlink" title="Bidirectional RNN"></a>Bidirectional RNN</h4><p>双向循环神经网络是包含两个不同走向循环神经网络的网络。正向RNN从前向后读取输入序列，逆向RNN反之。两个RNN互相交叠，输出由这两个RNN的隐藏层的状态决定。双向RNN主要被用于自然语言处理问题（例，处理一个单词需要考虑单词前后的单词）。相关文献</p>
<ul>
<li><a href="http://www.di.ufpe.br/~fnj/RNA/bibliografia/BRNN.pdf" target="_blank" rel="noopener">Bidirectional Recurrent Neural Networks</a></li>
</ul>
<h3 id="C"><a href="#C" class="headerlink" title="C"></a>C</h3><h4 id="Caffe"><a href="#Caffe" class="headerlink" title="Caffe"></a>Caffe</h4><p><a href="http://caffe.berkeleyvision.org/" target="_blank" rel="noopener">Caffe</a> 是 Berkeley Vision和Learning Center开发的深度学习框架，在处理视觉处理问题和CNN模型方面非常有用。</p>
<h4 id="Categorical-Cross-Entropy-Loss"><a href="#Categorical-Cross-Entropy-Loss" class="headerlink" title="Categorical Cross-Entropy Loss"></a>Categorical Cross-Entropy Loss</h4><p>分类交叉熵损失也被称作负对数似然，它是处理分类问题或者评估概率分布相似性的方法，特别是用于评估真值标签。其公式为$$L = -sum(y * log(y_prediction))$$，其中y是真标签的概率分布（独热向量），$$y_prediction$$是已预测标签的概率分布（一般使用softmax函数）</p>
<h4 id="Channel"><a href="#Channel" class="headerlink" title="Channel"></a>Channel</h4><p>深度学习模型的输入数据可以有多个通道。例如，图片有RGB三个通道。因此图片可以被一个3维张量表示，分别是通道、高度和宽度。自然语言处理数据也有多个通道的概念。例如，数据有不同类别的_<strong>嵌入</strong>_表示。</p>
<h4 id="Convolutional-Neural-Network-CNN-ConvNet"><a href="#Convolutional-Neural-Network-CNN-ConvNet" class="headerlink" title="Convolutional Neural Network (CNN, ConvNet)"></a>Convolutional Neural Network (CNN, ConvNet)</h4><p>卷积神经网络使用卷积层从输入数据中提取有效特征。通常卷积神经网络由卷积、池化和全连接层组成。因为其在视觉处理任务的出色表现，卷积神经网络近年来一直非常流行。相关文章：</p>
<ul>
<li><a href="http://cs231n.github.io/" target="_blank" rel="noopener">Stanford CS231n class – Convolutional Neural Networks for Visual Recognition</a></li>
<li><a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/" target="_blank" rel="noopener">Understanding Convolutional Neural Networks for NLP</a></li>
</ul>
<h3 id="D"><a href="#D" class="headerlink" title="D"></a>D</h3><h4 id="Deep-Belief-Network-DBN"><a href="#Deep-Belief-Network-DBN" class="headerlink" title="Deep Belief Network (DBN)"></a>Deep Belief Network (DBN)</h4><p>深度信念网络，通过无监督的概率图模型来学习数据特征。DBN由多个隐层组成，前后隐层的神经元间相互连接。每层神经网络由受限玻尔兹曼机组成，分别进行训练层。</p>
<ul>
<li><a href="https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf" target="_blank" rel="noopener">A fast learning algorithm for deep belief nets</a></li>
</ul>
<h4 id="Deep-Dream"><a href="#Deep-Dream" class="headerlink" title="Deep Dream"></a>Deep Dream</h4><p>Google发明的一项技术，对深度卷积神经网络学习到的数据进行提取，并用于生成新图片、修改图片甚至给图片加入梦幻般的效果。相关资料：</p>
<ul>
<li><a href="https://github.com/google/deepdream" target="_blank" rel="noopener">Deep Dream on Github</a></li>
<li><a href="http://googleresearch.blogspot.ch/2015/06/inceptionism-going-deeper-into-neural.html" target="_blank" rel="noopener">Inceptionism: Going Deeper into Neural Networks</a></li>
</ul>
<h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>随机失活是神经网络中用于避免过拟合的一种方法。最早被用于CNN网络，目前被广泛使用到其他神经网络中。相关资料：</p>
<ul>
<li><a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" target="_blank" rel="noopener">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></li>
<li><a href="http://arxiv.org/abs/1409.2329" target="_blank" rel="noopener">Recurrent Neural Network Regularization</a></li>
</ul>
<h3 id="E"><a href="#E" class="headerlink" title="E"></a>E</h3><h4 id="Embedding（嵌入）"><a href="#Embedding（嵌入）" class="headerlink" title="Embedding（嵌入）"></a>Embedding（嵌入）</h4><p>嵌入指的是将单词或者句子映射成向量形式。比较流行的嵌入是单词嵌入（例如，<a href="http://www.wildml.com/deep-learning-glossary/#word2vec" target="_blank" rel="noopener">word2vec</a> 或 <a href="http://www.wildml.com/deep-learning-glossary/#glove" target="_blank" rel="noopener">GloVe</a>）。我们也可以嵌入句子、段落或者图片。比如说，通过映射图片和他们的文字描述到嵌入空间来减少他们之间的距离，来将图片和对应的标签进行关联。嵌入可以单独进行（如采用<a href="http://www.wildml.com/deep-learning-glossary/#word2vec" target="_blank" rel="noopener">word2vec</a>），也可以作为某个机器学习任务的一部分，例如情感分析。通常，神经网络的输入均是已经训练和优化过的数据。</p>
<h4 id="Exploding-Gradient-Problem（梯度爆炸问题）"><a href="#Exploding-Gradient-Problem（梯度爆炸问题）" class="headerlink" title="Exploding Gradient Problem（梯度爆炸问题）"></a>Exploding Gradient Problem（梯度爆炸问题）</h4><p>梯度爆炸问题与梯度消失问题正好相反。在深度神经网络中，逆传播过程可能会造成梯度爆炸从而产生数字溢出。一种解决梯度爆炸的方式是<a href="http://www.wildml.com/deep-learning-glossary/#gradient-clipping" target="_blank" rel="noopener">梯度修剪</a></p>
<ul>
<li><a href="http://arxiv.org/abs/1211.5063" target="_blank" rel="noopener">On the difficulty of training recurrent neural networks</a></li>
</ul>
<h3 id="F"><a href="#F" class="headerlink" title="F"></a>F</h3><h4 id="Fine-Tuning"><a href="#Fine-Tuning" class="headerlink" title="Fine-Tuning"></a>Fine-Tuning</h4><p>优化调节指的是从另外的任务得到优化过的初始化学习参数。例如，使用<a href="http://www.wildml.com/deep-learning-glossary/#word2vec" target="_blank" rel="noopener">word2vec</a>对自然语言处理任务的单词做预处理</p>
<h3 id="G"><a href="#G" class="headerlink" title="G"></a>G</h3><h4 id="Gradient-Clipping"><a href="#Gradient-Clipping" class="headerlink" title="Gradient Clipping"></a>Gradient Clipping</h4><p>梯度修剪主要用于避免深度神经网络（特别是循环神经网络）的梯度爆炸问题。进行梯度修剪的方式有多种，一种常用的方式是对梯度进行L2正则化（new_gradients = gradients * threshold / l2_norm(gradients)），参考：</p>
<ul>
<li><a href="http://arxiv.org/abs/1211.5063" target="_blank" rel="noopener">On the difficulty of training recurrent neural networks</a></li>
</ul>
<h4 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h4><p>Glove是一种用于单词嵌入的无监督学习算法。Glove向量和wordvec用途相同，但是表示有差异，这是由于用于嵌入方式不同</p>
<ul>
<li><a href="http://nlp.stanford.edu/pubs/glove.pdf" target="_blank" rel="noopener">GloVe: Global Vectors for Word Representation</a></li>
</ul>
<h4 id="GoogleLeNet"><a href="#GoogleLeNet" class="headerlink" title="GoogleLeNet"></a>GoogleLeNet</h4><p>赢得2014年ILSVRC挑战的卷积神经网络框架。它使用记忆模块减少参数，同时提升对计算资源的有效利用率。参考：</p>
<ul>
<li><a href="http://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">Going Deeper with Convolutions</a></li>
</ul>
<h4 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h4><p>GRU( Gated Recurrent Unit ,门循环单元)是LSTM单元的简化形式，有更少的参数。类似于LSTM神经元，它使用门策略来避免梯度消失问题，使得RNN有效地学习长范围的关联。GRU内部有重置和更新门来决定旧的记忆是否需要保留还是要用当前时间的新值进行更新。参考：</p>
<ul>
<li><a href="http://arxiv.org/abs/1406.1078v3" target="_blank" rel="noopener">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a></li>
<li><a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/" target="_blank" rel="noopener">Recurrent Neural Network Tutorial, Part 4 – Implementing a GRU/LSTM RNN with Python and Theano</a></li>
</ul>
<h3 id="H"><a href="#H" class="headerlink" title="H"></a>H</h3><h4 id="Highway-Layer"><a href="#Highway-Layer" class="headerlink" title="Highway Layer"></a>Highway Layer</h4><p>Highway Layer (<a href="http://arxiv.org/abs/1505.00387" target="_blank" rel="noopener">论文参考</a>)是使用门策略来控制神经网络层信息流的机制。叠加使用多个Highway层可以训练非常深层次的神经网络。Highway通过门函数选择输入的那个部分通过以及那个部分需要通过变化函数处理。Highway层的基本公式为$$T * h(x) + (1 - T) * x$$，其中T是学习门函数，值位于0和1之间，h(x)是任意输入变化函数，x为输入数据。</p>
<h3 id="I"><a href="#I" class="headerlink" title="I"></a>I</h3><h4 id="ICML"><a href="#ICML" class="headerlink" title="ICML"></a>ICML</h4><p> <a href="http://icml.cc/" target="_blank" rel="noopener">International Conference for Machine Learning</a>, 机器学习领域顶级会议</p>
<h4 id="ILSVRC"><a href="#ILSVRC" class="headerlink" title="ILSVRC"></a>ILSVRC</h4><p><a href="http://www.image-net.org/challenges/LSVRC/" target="_blank" rel="noopener">ImageNet Large Scale Visual Recognition Challenge</a> 是图像识别分类领域最热门的竞赛。</p>
<h4 id="Inception-Module"><a href="#Inception-Module" class="headerlink" title="Inception Module"></a>Inception Module</h4><p>记忆单元用于卷积神经网络，提升网络的计算性能。参考：</p>
<ul>
<li><a href="http://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">Going Deeper with Convolutions</a></li>
</ul>
<h3 id="K"><a href="#K" class="headerlink" title="K"></a>K</h3><h4 id="KerasK"><a href="#KerasK" class="headerlink" title="KerasK"></a>Keras<a href="http://keras.io/" target="_blank" rel="noopener">K</a></h4><p><a href="http://keras.io/" target="_blank" rel="noopener">Keras</a>是包含对深度学习进行深度封装的Python库。可以在<a href="http://www.wildml.com/deep-learning-glossary/#tensorflow" target="_blank" rel="noopener">TensorFlow</a>, <a href="http://www.wildml.com/deep-learning-glossary/#theano" target="_blank" rel="noopener">Theano</a>, 或 <a href="https://github.com/Microsoft/CNTK" target="_blank" rel="noopener">CNTK</a>上层使用</p>
<h3 id="L"><a href="#L" class="headerlink" title="L"></a>L</h3><h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p>长期短记忆网络主要用记忆门来避免RNN网络的梯度消失问题。利用LSTM单元计算RNN隐层，可以有效的传递梯度以及学习长范围关联。参考：</p>
<ul>
<li><a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf" target="_blank" rel="noopener">Long Short-Term Memory</a></li>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a></li>
<li><a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/" target="_blank" rel="noopener">Recurrent Neural Network Tutorial, Part 4 – Implementing a GRU/LSTM RNN with Python and Theano</a></li>
</ul>
<h3 id="M"><a href="#M" class="headerlink" title="M"></a>M</h3><h4 id="Max-Pooling"><a href="#Max-Pooling" class="headerlink" title="Max-Pooling"></a>Max-Pooling</h4><p>最大池化是卷积神经网络的一种池化操作，池化时选择特征片段里的最大值，是卷积神经网络的常用池化操作。</p>
<h3 id="M-1"><a href="#M-1" class="headerlink" title="M"></a>M</h3><h4 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h4><p> MNIST 数据集 是最常使用的图像识别数据集了。基本上也是许多机器学习课程的范例数据集，更多介绍直接参考<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">官网</a>即可。</p>
<h4 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h4><p>Momentum是梯度下降算法的扩展，加速和优化了参数更新过程。实际使用中，加入momentum到梯度下降中，可以使深度网络得到更好的收敛 。参考：</p>
<ul>
<li><a href="http://www.nature.com/nature/journal/v323/n6088/abs/323533a0.html" target="_blank" rel="noopener">Learning representations by back-propagating errors</a></li>
</ul>
<h4 id="Multilayer-Perceptron-MLP"><a href="#Multilayer-Perceptron-MLP" class="headerlink" title="Multilayer Perceptron (MLP)"></a>Multilayer Perceptron (MLP)</h4><p>多层感知是一种多全连接层的前馈神经网络，使用<strong>激活函数</strong>处理数据做非线性化。MLP是多层神经网络或深度神经网络的最基本形式。</p>
<h3 id="N"><a href="#N" class="headerlink" title="N"></a>N</h3><h4 id="Negative-Log-Likelihood-NLL"><a href="#Negative-Log-Likelihood-NLL" class="headerlink" title="Negative Log Likelihood (NLL)"></a>Negative Log Likelihood (NLL)</h4><p>见 <a href="http://www.wildml.com/deep-learning-glossary/#ce-loss" target="_blank" rel="noopener">Categorical Cross Entropy Loss</a>.</p>
<h4 id="Neural-Machine-Translation-NMT"><a href="#Neural-Machine-Translation-NMT" class="headerlink" title="Neural Machine Translation (NMT)"></a>Neural Machine Translation (NMT)</h4><p>神经机器翻译指的是使用神经网络来翻译语言。参考：</p>
<ul>
<li><a href="http://arxiv.org/abs/1409.3215" target="_blank" rel="noopener">Sequence to sequence learning with neural networks</a></li>
<li><a href="http://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a></li>
</ul>
<h4 id="Neural-Turing-Machine-NTM"><a href="#Neural-Turing-Machine-NTM" class="headerlink" title="Neural Turing Machine (NTM)"></a>Neural Turing Machine (NTM)</h4><p>神经图灵机可以从范例中推导简单的算法。例如，NTM可以从输入输出范例中学习分类算法。在程序运行时神经图灵机通常可以学习到一些处理状态的记忆方法</p>
<ul>
<li><a href="http://arxiv.org/abs/1410.5401" target="_blank" rel="noopener">Neural Turing Machines</a></li>
</ul>
<h4 id="Nonlinearity（去线性化）"><a href="#Nonlinearity（去线性化）" class="headerlink" title="Nonlinearity（去线性化）"></a>Nonlinearity（去线性化）</h4><p>见<strong><em>激活函数</em></strong>.</p>
<h4 id="Noise-contrastive-estimation-NCE"><a href="#Noise-contrastive-estimation-NCE" class="headerlink" title="Noise-contrastive estimation (NCE)"></a>Noise-contrastive estimation (NCE)</h4><p>噪声对比评估是一种在大量词汇表输出常见用来训练分类器的损失抽样方法。通过计算所有可能分类的Softmax是代价昂贵的。而使用NCE，可以有效减少二分类问题的代价，而只需要通过从“真”分布和人工产生的噪声分布区来训练分类器。例如：</p>
<ul>
<li><a href="http://www.jmlr.org/proceedings/papers/v9/gutmann10a/gutmann10a.pdf" target="_blank" rel="noopener">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</a></li>
<li><a href="http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf" target="_blank" rel="noopener">Learning word embeddings efficiently with noise-contrastive estimation</a></li>
</ul>
<h3 id="P"><a href="#P" class="headerlink" title="P"></a>P</h3><h4 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h4><p>见<strong><em>最大池化</em></strong>和<strong><em>平均池化</em></strong>.</p>
<h4 id="Restricted-Boltzmann-Machine-RBN"><a href="#Restricted-Boltzmann-Machine-RBN" class="headerlink" title="Restricted Boltzmann Machine (RBN)"></a>Restricted Boltzmann Machine (RBN)</h4><p>受限玻尔兹曼机是深度信念网络使用的一种概率图模型。参考：</p>
<ul>
<li><a href="http://www-psych.stanford.edu/~jlm/papers/PDP/Volume%201/Chap6_PDP86.pdf" target="_blank" rel="noopener">Chapter 6: Information Processing in Dynamical Systems: Foundations of Harmony Theory</a></li>
<li><a href="http://image.diku.dk/igel/paper/AItRBM-proof.pdf" target="_blank" rel="noopener">An Introduction to Restricted Boltzmann Machines</a></li>
</ul>
<h3 id="R"><a href="#R" class="headerlink" title="R"></a>R</h3><h4 id="Recurrent-Neural-Network-RNN"><a href="#Recurrent-Neural-Network-RNN" class="headerlink" title="Recurrent Neural Network (RNN)"></a>Recurrent Neural Network (RNN)</h4><p>RNN代表循环神经网络，参考<a href="https://www.l2h.site/2019/07/13/%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9crnn%e7%ae%80%e5%8d%95%e7%90%86%e8%a7%a3/" target="_blank" rel="noopener">本站文章</a>，或者：</p>
<ul>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a></li>
<li><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" target="_blank" rel="noopener">Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs</a></li>
</ul>
<h4 id="Recursive-Neural-Network"><a href="#Recursive-Neural-Network" class="headerlink" title="Recursive Neural Network"></a>Recursive Neural Network</h4><p>递归神经网络是循环神经网络的一种树状形式。详情参见：</p>
<ul>
<li><a href="http://ai.stanford.edu/~ang/papers/icml11-ParsingWithRecursiveNeuralNetworks.pdf" target="_blank" rel="noopener">Parsing Natural Scenes and Natural Language with Recursive Neural Networks</a></li>
</ul>
<h4 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h4><p>线性整流函数（Rectified Linear Unit）是一种激活函数，深度学习中做去线性化处理。参考：</p>
<ul>
<li><a href="http://arxiv.org/abs/1502.01852" target="_blank" rel="noopener">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></li>
<li><a href="http://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf" target="_blank" rel="noopener">Rectifier Nonlinearities Improve Neural Network Acoustic Models</a></li>
<li><a href="http://www.cs.toronto.edu/~fritz/absps/reluICML.pdf" target="_blank" rel="noopener">Rectified Linear Units Improve Restricted Boltzmann Machines</a></li>
</ul>
<h4 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h4><p>深度残差网络（Deep Residual Network），赢得了ILSVRC 2015挑战赛，参考。</p>
<ul>
<li><a href="http://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">Deep Residual Learning for Image Recognition</a></li>
</ul>
<h4 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h4><p>RMSProp是一种基于梯度的优化算法。具体算法介绍见 ：</p>
<ul>
<li><a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" target="_blank" rel="noopener">Neural Networks for Machine Learning Lecture 6a</a></li>
<li><a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="noopener">Stanford CS231n: Optimization Algorithms</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a></li>
</ul>
<h3 id="S"><a href="#S" class="headerlink" title="S"></a>S</h3><h4 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h4><p>序列到序列模型读取血量作为输入，产生另外一个序列作为输出。与RNN不同的地方是，在产生输出之前，输入序列被一次性完整的输入。一般使用两个RNN实现，经典应用为机器翻译、编解码等，参考：</p>
<ul>
<li><a href="http://arxiv.org/abs/1409.3215" target="_blank" rel="noopener">Sequence to Sequence Learning with Neural Networks</a></li>
</ul>
<h4 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h4><p>随机梯度下降是一种有效的梯度优化算法(<a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank" rel="noopener">Wikipedia</a>)，其扩展算法包括 <strong>Momentum</strong>, <strong>Adagrad</strong>, <strong>rmsprop</strong>, <strong>Adadelta</strong> 以及 <strong>Adam</strong>.参考：</p>
<ul>
<li><a href="http://www.magicbroom.info/Papers/DuchiHaSi10.pdf" target="_blank" rel="noopener">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a></li>
<li><a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="noopener">Stanford CS231n: Optimization Algorithms</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a></li>
</ul>
<h4 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h4><p><a href="https://baike.baidu.com/item/Softmax%E5%87%BD%E6%95%B0/22772270?fr=aladdin" target="_blank" rel="noopener">Softmax函数</a>，它能将一个含任意实数的K维向量 “压缩”到另一个K维实向量 中，使得每一个元素的范围都在 （0,1）之间，并且所有元素的和为1。主要作为处理分类问题的输出</p>
<h3 id="T"><a href="#T" class="headerlink" title="T"></a>T</h3><h4 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h4><p><a href="https://www.tensorflow.org/" target="_blank" rel="noopener">TensorFlow</a> 是Google提供的开源深度学习框架，可以对数据流图做计算，也封装了现行主要的神经网络运算。支持C++/Python.</p>
<h4 id="Theano"><a href="#Theano" class="headerlink" title="Theano"></a>Theano</h4><p><a href="http://deeplearning.net/software/theano/" target="_blank" rel="noopener">Theano</a> 是一个封装深度神经网络算法的Python库</p>
<h3 id="V"><a href="#V" class="headerlink" title="V"></a>V</h3><h4 id="Vanishing-Gradient-Problem"><a href="#Vanishing-Gradient-Problem" class="headerlink" title="Vanishing Gradient Problem"></a>Vanishing Gradient Problem</h4><p>梯度消失问题在深度神经网络学习中越来越常见，特别是循环神经网络，使用较小的梯度（位于0和1直接）。因为梯度在逆传播过程中会相乘，所以会在层与层传递间逐渐“消失”，导致长范围的关联消失。解决方法主要有使用<strong>ReLU</strong>激活，或者使用改进网络<strong><em>LSTM</em></strong>等。参考：</p>
<ul>
<li><a href="http://www.jmlr.org/proceedings/papers/v28/pascanu13.pdf" target="_blank" rel="noopener">On the difficulty of training recurrent neural networks</a></li>
</ul>
<h4 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h4><p>VGG是赢得2014 ImageNet定位和分类跟踪问题第一二名的卷积神经网络。它由16到19个权重层和1*1或3*3的小卷积过滤器组成。参考：</p>
<ul>
<li><a href="http://arxiv.org/abs/1409.1556" target="_blank" rel="noopener">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></li>
</ul>
<h3 id="W"><a href="#W" class="headerlink" title="W"></a>W</h3><h4 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h4><p>word2vec用于单词嵌入的算法，参考：</p>
<ul>
<li><a href="http://arxiv.org/abs/1301.3781" target="_blank" rel="noopener">Efficient Estimation of Word Representations in Vector Space</a></li>
<li><a href="http://arxiv.org/abs/1310.4546" target="_blank" rel="noopener">Distributed Representations of Words and Phrases and their Compositionality</a></li>
<li><a href="http://arxiv.org/abs/1411.2738" target="_blank" rel="noopener">word2vec Parameter Learning Explained</a></li>
</ul>

    </div>

    
    
    
      

        
      

      <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2019/07/14/e6-9c-ba-e5-99-a8-e5-ad-a6-e4-b9-a0-e5-87-a0-e4-b8-aa-e8-a6-81-e7-b4-a0/" rel="next" title="机器学习几个要素">
                  <i class="fa fa-chevron-left"></i> 机器学习几个要素
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2019/07/19/word-embedding-model/" rel="prev" title="Word Embedding（词嵌入）模型介绍">
                  Word Embedding（词嵌入）模型介绍 <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    
  <div class="comments" id="comments">
    <div id="SOHUCS"></div>
  </div>
  
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#A"><span class="nav-number">1.</span> <span class="nav-text">A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Activation-Function（激活函数）"><span class="nav-number">1.1.</span> <span class="nav-text">Activation Function（激活函数）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adadelta"><span class="nav-number">1.2.</span> <span class="nav-text">Adadelta</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adagrad"><span class="nav-number">1.3.</span> <span class="nav-text">Adagrad</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adam"><span class="nav-number">1.4.</span> <span class="nav-text">Adam</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Affine-Layer"><span class="nav-number">1.5.</span> <span class="nav-text">Affine Layer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Attention-Mechanism"><span class="nav-number">1.6.</span> <span class="nav-text">Attention Mechanism</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Alexnet"><span class="nav-number">1.7.</span> <span class="nav-text">Alexnet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Autoencoder"><span class="nav-number">1.8.</span> <span class="nav-text">Autoencoder</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Average-Pooling"><span class="nav-number">1.9.</span> <span class="nav-text">Average-Pooling</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B"><span class="nav-number">2.</span> <span class="nav-text">B</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Backpropagation（逆传播，后向传播）"><span class="nav-number">2.1.</span> <span class="nav-text">Backpropagation（逆传播，后向传播）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Backpropagation-Through-Time-BPTT"><span class="nav-number">2.2.</span> <span class="nav-text">Backpropagation Through Time (BPTT)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Batch-Normalization"><span class="nav-number">2.3.</span> <span class="nav-text">Batch Normalization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bidirectional-RNN"><span class="nav-number">2.4.</span> <span class="nav-text">Bidirectional RNN</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C"><span class="nav-number">3.</span> <span class="nav-text">C</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Caffe"><span class="nav-number">3.1.</span> <span class="nav-text">Caffe</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Categorical-Cross-Entropy-Loss"><span class="nav-number">3.2.</span> <span class="nav-text">Categorical Cross-Entropy Loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Channel"><span class="nav-number">3.3.</span> <span class="nav-text">Channel</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Convolutional-Neural-Network-CNN-ConvNet"><span class="nav-number">3.4.</span> <span class="nav-text">Convolutional Neural Network (CNN, ConvNet)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#D"><span class="nav-number">4.</span> <span class="nav-text">D</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Deep-Belief-Network-DBN"><span class="nav-number">4.1.</span> <span class="nav-text">Deep Belief Network (DBN)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Deep-Dream"><span class="nav-number">4.2.</span> <span class="nav-text">Deep Dream</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dropout"><span class="nav-number">4.3.</span> <span class="nav-text">Dropout</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#E"><span class="nav-number">5.</span> <span class="nav-text">E</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Embedding（嵌入）"><span class="nav-number">5.1.</span> <span class="nav-text">Embedding（嵌入）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Exploding-Gradient-Problem（梯度爆炸问题）"><span class="nav-number">5.2.</span> <span class="nav-text">Exploding Gradient Problem（梯度爆炸问题）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#F"><span class="nav-number">6.</span> <span class="nav-text">F</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Fine-Tuning"><span class="nav-number">6.1.</span> <span class="nav-text">Fine-Tuning</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#G"><span class="nav-number">7.</span> <span class="nav-text">G</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient-Clipping"><span class="nav-number">7.1.</span> <span class="nav-text">Gradient Clipping</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GloVe"><span class="nav-number">7.2.</span> <span class="nav-text">GloVe</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GoogleLeNet"><span class="nav-number">7.3.</span> <span class="nav-text">GoogleLeNet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GRU"><span class="nav-number">7.4.</span> <span class="nav-text">GRU</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#H"><span class="nav-number">8.</span> <span class="nav-text">H</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Highway-Layer"><span class="nav-number">8.1.</span> <span class="nav-text">Highway Layer</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I"><span class="nav-number">9.</span> <span class="nav-text">I</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ICML"><span class="nav-number">9.1.</span> <span class="nav-text">ICML</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ILSVRC"><span class="nav-number">9.2.</span> <span class="nav-text">ILSVRC</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Inception-Module"><span class="nav-number">9.3.</span> <span class="nav-text">Inception Module</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K"><span class="nav-number">10.</span> <span class="nav-text">K</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#KerasK"><span class="nav-number">10.1.</span> <span class="nav-text">KerasK</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#L"><span class="nav-number">11.</span> <span class="nav-text">L</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#LSTM"><span class="nav-number">11.1.</span> <span class="nav-text">LSTM</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#M"><span class="nav-number">12.</span> <span class="nav-text">M</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Max-Pooling"><span class="nav-number">12.1.</span> <span class="nav-text">Max-Pooling</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#M-1"><span class="nav-number">13.</span> <span class="nav-text">M</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MNIST"><span class="nav-number">13.1.</span> <span class="nav-text">MNIST</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Momentum"><span class="nav-number">13.2.</span> <span class="nav-text">Momentum</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multilayer-Perceptron-MLP"><span class="nav-number">13.3.</span> <span class="nav-text">Multilayer Perceptron (MLP)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#N"><span class="nav-number">14.</span> <span class="nav-text">N</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Negative-Log-Likelihood-NLL"><span class="nav-number">14.1.</span> <span class="nav-text">Negative Log Likelihood (NLL)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Neural-Machine-Translation-NMT"><span class="nav-number">14.2.</span> <span class="nav-text">Neural Machine Translation (NMT)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Neural-Turing-Machine-NTM"><span class="nav-number">14.3.</span> <span class="nav-text">Neural Turing Machine (NTM)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Nonlinearity（去线性化）"><span class="nav-number">14.4.</span> <span class="nav-text">Nonlinearity（去线性化）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Noise-contrastive-estimation-NCE"><span class="nav-number">14.5.</span> <span class="nav-text">Noise-contrastive estimation (NCE)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#P"><span class="nav-number">15.</span> <span class="nav-text">P</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Pooling"><span class="nav-number">15.1.</span> <span class="nav-text">Pooling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Restricted-Boltzmann-Machine-RBN"><span class="nav-number">15.2.</span> <span class="nav-text">Restricted Boltzmann Machine (RBN)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#R"><span class="nav-number">16.</span> <span class="nav-text">R</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Recurrent-Neural-Network-RNN"><span class="nav-number">16.1.</span> <span class="nav-text">Recurrent Neural Network (RNN)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Recursive-Neural-Network"><span class="nav-number">16.2.</span> <span class="nav-text">Recursive Neural Network</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ReLU"><span class="nav-number">16.3.</span> <span class="nav-text">ReLU</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ResNet"><span class="nav-number">16.4.</span> <span class="nav-text">ResNet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RMSProp"><span class="nav-number">16.5.</span> <span class="nav-text">RMSProp</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#S"><span class="nav-number">17.</span> <span class="nav-text">S</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Seq2Seq"><span class="nav-number">17.1.</span> <span class="nav-text">Seq2Seq</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SGD"><span class="nav-number">17.2.</span> <span class="nav-text">SGD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Softmax"><span class="nav-number">17.3.</span> <span class="nav-text">Softmax</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#T"><span class="nav-number">18.</span> <span class="nav-text">T</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#TensorFlow"><span class="nav-number">18.1.</span> <span class="nav-text">TensorFlow</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Theano"><span class="nav-number">18.2.</span> <span class="nav-text">Theano</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#V"><span class="nav-number">19.</span> <span class="nav-text">V</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Vanishing-Gradient-Problem"><span class="nav-number">19.1.</span> <span class="nav-text">Vanishing Gradient Problem</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#VGG"><span class="nav-number">19.2.</span> <span class="nav-text">VGG</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#W"><span class="nav-number">20.</span> <span class="nav-text">W</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#word2vec"><span class="nav-number">20.1.</span> <span class="nav-text">word2vec</span></a></li></ol></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.jpg"
      alt="Lambert">
  <p class="site-author-name" itemprop="name">Lambert</p>
  <div class="site-description" itemprop="description">旅行日记，Linux技术，机器学习，建站技巧</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">128</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">113</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/lambertdev" title="GitHub &rarr; https://github.com/lambertdev" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:lambertdev@gmail.com" title="E-Mail &rarr; mailto:lambertdev@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      友情链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.echoteen.com/" title="https://www.echoteen.com/" rel="noopener" target="_blank">Echo少年</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://mlldxe.cn/" title="http://mlldxe.cn/" rel="noopener" target="_blank">Mlldxe’s Blog</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://oldpan.me/" title="https://oldpan.me/" rel="noopener" target="_blank">Oldpan的个人博客</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://www.moerats.com/" title="https://www.moerats.com/" rel="noopener" target="_blank">Rat's Blog</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://www.supert.net.cn/" title="http://www.supert.net.cn/" rel="noopener" target="_blank">七年。</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://www.gazyip.cn/" title="https://www.gazyip.cn/" rel="noopener" target="_blank">八个比特</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://www.sharexbar.com/" title="http://www.sharexbar.com/" rel="noopener" target="_blank">分享巴中</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://blog.isoyu.com/" title="https://blog.isoyu.com/" rel="noopener" target="_blank">姬长信</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://masuit.com/" title="https://masuit.com/" rel="noopener" target="_blank">懒得勤快博客</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://www.zahuiw.com/" title="http://www.zahuiw.com/" rel="noopener" target="_blank">杂烩网</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://www.yangqq.com/" title="http://www.yangqq.com/" rel="noopener" target="_blank">杨青-"青姐"</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://www.yanshisan.cn/" title="https://www.yanshisan.cn/" rel="noopener" target="_blank">燕十三</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://www.tianshan277.com/" title="http://www.tianshan277.com/" rel="noopener" target="_blank">田珊珊个人博客</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://www.52ecy.cn/" title="https://www.52ecy.cn/" rel="noopener" target="_blank">阿珏博客</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://asciiflow.com/" title="http://asciiflow.com/" rel="noopener" target="_blank">在线ASCii绘图</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://daohang.lusongsong.com/" title="http://daohang.lusongsong.com/" rel="noopener" target="_blank">博客大全</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://travel-hare.l2h.site/" title="http://travel-hare.l2h.site/" rel="noopener" target="_blank">旅行的兔撸撸</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://neuralnetworksanddeeplearning.com/" title="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">神经网络深度学习</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://pin.l2h.site" title="https://pin.l2h.site" rel="noopener" target="_blank">惠拼购</a>
        </li>
      
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright"><a href="http://www.beian.miit.gov.cn" rel="noopener" target="_blank">蜀ICP备17001636号-1 </a>
  <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=" rel="noopener" target="_blank"> </a>&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lambert</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a></div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a></div>

        












        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.0"></script><script src="/js/motion.js?v=7.4.0"></script>
<script src="/js/schemes/muse.js?v=7.4.0"></script>
<script src="/js/next-boot.js?v=7.4.0"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  

  <script>
  (function() {
    var appid = 'cyutixI06';
    var conf = 'prod_3e046acc8875533ad4449ee8d8e0fb14';
    var width = window.innerWidth || document.documentElement.clientWidth;
    if (width < 960) {
      window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>');
    } else {
      var loadJs=function(d,a){var c=document.getElementsByTagName("head")[0]||document.head||document.documentElement;var b=document.createElement("script");b.setAttribute("type","text/javascript");b.setAttribute("charset","UTF-8");b.setAttribute("src",d);if(typeof a==="function"){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e==="loaded"||e==="complete"){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs("https://changyan.sohu.com/upload/changyan.js",function(){window.changyan.api.config({appid:appid,conf:conf})});
    }
  })();
  </script>
  <script src="https://assets.changyan.sohu.com/upload/plugins/plugins.count.js"></script>

</body>
</html>
