<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico?v=7.4.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.0" color="#222">
  <link rel="alternate" href="/atom.xml" title="L&H SITE" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css?v=7.4.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.4.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: true,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    },
    sidebarPadding: 40
  };
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=IntersectionObserver"></script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-3852401481783193",
    enable_page_level_ads: true
  });
</script>

  <meta name="description" content="前言机器学习，神经网络，已经不算前言的名词、未来的趋势。未来大部分的工作都可能会被机器所取代，这不是危言耸听。秉承着不能被机器淘汰的思想，必须了解到机器到底在学习什么，怎么能学习。本系列文算是个人对机器学习的一个笔记，加上自己的一些总结和变化。记录的过程即是加深学习的过程，希望可以今年内将这个系列学习完，也希望对访问博客的朋友们有所帮助。本文大部分内容引用、整理或翻译自Michael Nielse">
<meta name="keywords" content="机器学习,神经网络,Neural Network">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络和深度学习(1)">
<meta property="og:url" content="http://l2h.site/2019/02/02/machine-learning-neural-network-1/index.html">
<meta property="og:site_name" content="L&amp;H SITE">
<meta property="og:description" content="前言机器学习，神经网络，已经不算前言的名词、未来的趋势。未来大部分的工作都可能会被机器所取代，这不是危言耸听。秉承着不能被机器淘汰的思想，必须了解到机器到底在学习什么，怎么能学习。本系列文算是个人对机器学习的一个笔记，加上自己的一些总结和变化。记录的过程即是加深学习的过程，希望可以今年内将这个系列学习完，也希望对访问博客的朋友们有所帮助。本文大部分内容引用、整理或翻译自Michael Nielse">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://pic.l2h.site/Machine-Learning-Book.jpg">
<meta property="og:image" content="http://neuralnetworksanddeeplearning.com/images/digits.png">
<meta property="og:image" content="http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png">
<meta property="og:image" content="http://pic.l2h.site/Nueral-Cell-1024x768.jpeg">
<meta property="og:image" content="http://pic.l2h.site/neural-network-1-1.jpeg">
<meta property="og:image" content="http://pic.l2h.site/neural-network-1-2.jpg">
<meta property="og:image" content="http://pic.l2h.site/perceptron-func-0.png">
<meta property="og:image" content="http://pic.l2h.site/perceptron-func-1.png">
<meta property="og:image" content="http://neuralnetworksanddeeplearning.com/images/digits.png">
<meta property="og:image" content="http://neuralnetworksanddeeplearning.com/images/digits_separate.png">
<meta property="og:image" content="http://neuralnetworksanddeeplearning.com/images/tikz12.png">
<meta property="og:image" content="http://neuralnetworksanddeeplearning.com/images/valley.png">
<meta property="og:image" content="http://neuralnetworksanddeeplearning.com/images/valley_with_ball.png">
<meta property="og:updated_time" content="2020-08-23T06:17:20.525Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="神经网络和深度学习(1)">
<meta name="twitter:description" content="前言机器学习，神经网络，已经不算前言的名词、未来的趋势。未来大部分的工作都可能会被机器所取代，这不是危言耸听。秉承着不能被机器淘汰的思想，必须了解到机器到底在学习什么，怎么能学习。本系列文算是个人对机器学习的一个笔记，加上自己的一些总结和变化。记录的过程即是加深学习的过程，希望可以今年内将这个系列学习完，也希望对访问博客的朋友们有所帮助。本文大部分内容引用、整理或翻译自Michael Nielse">
<meta name="twitter:image" content="http://pic.l2h.site/Machine-Learning-Book.jpg">
  <link rel="canonical" href="http://l2h.site/2019/02/02/machine-learning-neural-network-1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>神经网络和深度学习(1) | L&H SITE</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d8b880a7a50fe4f306c58655c0c8db8a";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">L&H SITE</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">两个背包旅行者的网络自留地。分享旅行日记，Linux技术，机器学习，建站技巧</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-works">
      
    

    <a href="/works/" rel="section"><i class="fa fa-fw fa-th"></i>works</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-sitemap">
      
    

    <a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>Sitemap</a>

  </li>
  </ul>

    

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/lambertdev" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="http://l2h.site/2019/02/02/machine-learning-neural-network-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lambert">
      <meta itemprop="description" content="旅行日记，Linux技术，机器学习，建站技巧">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="L&H SITE">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">神经网络和深度学习(1)

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-02-02 18:36:44" itemprop="dateCreated datePublished" datetime="2019-02-02T18:36:44+00:00">2019-02-02</time>
            </span>
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Changyan: </span>
    
    
      <a title="changyan" href="/2019/02/02/machine-learning-neural-network-1/#SOHUCS" itemprop="discussionUrl"><span id="changyan_count_unit" class="post-comments-count hc-comment-count" data-xid="2019/02/02/machine-learning-neural-network-1/" itemprop="commentCount"></span></a>
    
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>机器学习，神经网络，已经不算前言的名词、未来的趋势。未来大部分的工作都可能会被机器所取代，这不是危言耸听。秉承着不能被机器淘汰的思想，必须了解到机器到底在学习什么，怎么能学习。本系列文算是个人对机器学习的一个笔记，加上自己的一些总结和变化。记录的过程即是加深学习的过程，希望可以今年内将这个系列学习完，也希望对访问博客的朋友们有所帮助。本文大部分内容引用、整理或翻译自<a href="http://michaelnielsen.org/" target="_blank" rel="noopener">Michael Nielsen</a>的《<a href="http://neuralnetworksanddeeplearning.com/index.html" target="_blank" rel="noopener">Neural Networks and Deep Learning</a> 》，已征得原作者许可，转载此文请先与我联系。作者原声明如下：</p><a id="more"></a>
<blockquote>
<p>This work is licensed under a <a href="http://creativecommons.org/licenses/by-nc/3.0/deed.en_GB" target="_blank" rel="noopener">Creative Commons Attribution-NonCommercial 3.0 Unported License</a>. This means you’re free to copy, share, and build on this book, but not to sell it. If you’re interested in<br>commercial use, please <a href="mailto:mn@michaelnielsen.org" target="_blank" rel="noopener">contact me</a>.</p>
</blockquote>
<p><img src="http://pic.l2h.site/Machine-Learning-Book.jpg" alt></p>
<p>本文主要内容包含：</p>
<ul>
<li>神经网络： 一个从生物学得到启发的漂亮的编程方法，让电脑可以从可观察的数据中进行学习</li>
<li>深度学习：运用神经网络机型学习的极为有效的技术体系</li>
</ul>
<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>神经网络和深度学习在处理图像识别、音频识别以及自然语言处理问题时能提供最佳的解决方案。 例如：人类可以很轻易地识别如下手写数字序列（基本上算是很直觉地识别出）。但是对计算机来讲，却是很复杂的事情，且无法运用人类的识别方式。</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/digits.png" alt></p>
<p>Fig 1. 手写字体</p>
<p>一般来说计算机识别步骤为：</p>
<ul>
<li>准备一系列已知的手写数据范例（如下图），称为训练数据</li>
<li>创造一个识别系统，从这些手写数据范例中归纳出识别的模式。</li>
</ul>
<p><img src="http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png" alt></p>
<p>Fig 2. 手写字体范例</p>
<h3 id="感知器"><a href="#感知器" class="headerlink" title="感知器"></a>感知器</h3><p>感知器是神经网络的基本结构，而神经网络正是参照了人类神经系统的特性构造起来的，其基本单元为_感知器_。要了解感知器，先看一张人类神经元的结构图。</p>
<p><img src="http://pic.l2h.site/Nueral-Cell-1024x768.jpeg" alt></p>
<p>Fig 3. 神经元</p>
<p>神经元的解释如下（来自<a href="https://baike.baidu.com/item/%E7%A5%9E%E7%BB%8F%E5%85%83/674777?fr=aladdin" target="_blank" rel="noopener">百度百科</a>）：</p>
<blockquote>
<p>神经细胞是神经系统最基本的结构和功能单位。分为细胞体和突起两部分。细胞体由细胞核、细胞膜、细胞质组成，具有联络和整合输入信息并传出信息的作用。突起有树突和轴突两种。树突短而分枝多，直接由细胞体扩张突出，形成树枝状，其作用是<strong>接受其他神经元轴突传来的冲动并传给细胞体</strong>。轴突长而分枝少，为粗细均匀的细长突起，常起于轴丘，<strong>其作用是接受外来刺激，再由细胞体传出</strong>。</p>
</blockquote>
<p>可见，其主要作用就是接受其他神经元的刺激，并传导到其他神经元。可以理解为神经元为多输入输出的一个基本单元。神经网络中的感知器工作原理也类同，如下图：它接受其他感知器的输出作为输入，乘上一定的权值加总后得到一个值。用函数$$f$$对该加总值进行处理，得到输出$$O$$。这个输出值是一个二元值，0或1，类比表示神经元是否产生输出刺激。公式如下：</p>
<p>$$ \begin{eqnarray} \mbox{output} &amp; = &amp; \left\{ \begin{array}{ll} 0 &amp; \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \\ 1 &amp; \mbox{if } \sum_j w_j x_j &gt; \mbox{ threshold} \end{array} \right. \tag{1}\end{eqnarray} $$</p>
<p>令 $$ b = \mbox{-threshold} $$，即有：</p>
<p>$$ \begin{eqnarray} \mbox{output} &amp; = &amp; \left\{ \begin{array}{ll} 0 &amp; \mbox{if } \sum_j w_j x_j +b\leq \mbox{0} \\ 1 &amp; \mbox{if } \sum_j w_j x_j+b &gt; \mbox{ 0} \end{array} \right. \tag{2}\end{eqnarray} $$</p>
<p>如何理解上述公式？通常人类要做一件事情的时候，一定会有很多条件的输入。每个条件都有一定的优先级，综合考虑每个条件之后，做出最后的判断。例如：老王想要决定晚上出不出去健身，影响他判断的条件可能有晚上会不会加班、今天天冷不冷、昨天有没有建过身等。</p>
<p>在机器学习中，这些输入条件即可用$$x_1$$到$$x_i$$表示，而每个条件对做出最后决策的影响成都即 $$w_1$$到$$w_i$$ 。这些数据的加总和通常不会是1或者0这样的结果，因此无法直接用来判断输出的结果。通常，上述公式感知器会将该加总和与阈值threshold进行比较。如果大于阈值则输出1，反之则输出0。</p>
<p><img src="http://pic.l2h.site/neural-network-1-1.jpeg" alt></p>
<p>Fig 4. Perceptron(感知器)</p>
<h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p>神经网络即由上述感知器作为神经元的网络（如Fig 5）。其基本组成为：</p>
<ul>
<li>感知器：即下图每一个小圆，神经网络的“神经元”</li>
<li>输入层：由N个感知器组成的层次（N为输入条件的个数）</li>
<li>输出层：有M个感知器组成的层次，（N为机器学习问题输出值的个数。对手写数字识别来讲，即有10种可能输出，即输出层由10个感知器组成）</li>
<li>多个隐层（Hidden Layer）： 非输入层也非输出层的中间层。可以为0个或者多个。</li>
<li>层与层连线上的参数 ，即感知器的权值 $$ w_{ij} $$ 和Threshold $$ b $$</li>
</ul>
<p>输入层和输出层中感知器的数量、隐层及其所包含感知器的数量，取决于要解决的机器学习问题。往往这些数量以及 $$ w_{ij} $$ 等变量初始值的选择，也决定着机器学习的效率以及效果，之后会再探讨。</p>
<p><img src="http://pic.l2h.site/neural-network-1-2.jpg" alt></p>
<p>Fig 5. Neural Network(神经网络)</p>
<p>机器学习的过程，就是通过训练数据，反推出神经网络层与层之间<strong>所有</strong>的权值<br>$$ w_{ij} $$ 与阈值 $$ b $$ ，直到所有检验数据达到需要的精准度。</p>
<h3 id="Sigmoid神经元"><a href="#Sigmoid神经元" class="headerlink" title="Sigmoid神经元"></a>Sigmoid神经元</h3><p><strong>感知器</strong>一节提到的函数，只是简单地从输入端加权值减去阈值的结果来判断输出。<br>例，当_水温能达到28_度，今天就会出去游泳;又或，当_小黑心情不好时_，就睡觉。 这样的作法很直觉，也比较符合我们的思考习惯。这种函数，非0即1，不够平滑，函数输出值随着输入的跳变较大。如下图， 当$$ \sum_j w_j x_j +b\ $$超过0之后，输出就会直接跳变为1。这样造成的结果是，输入中的某一个权值轻微变化，造成输出结果发生剧变。</p>
<p><img src="http://pic.l2h.site/perceptron-func-0.png" alt></p>
<p>Fig 6. 函数(2)的图形</p>
<p>而机器学习的过程，希望的是输入值的变化对输出值的变化不要产生过大的变动。即上述函数的变化较为平滑。而Sigmoid函数正式这样的函数，它的公式如下：</p>
<p>$$f\equiv\frac{1}{1+e^{-{(W \bullet X-b)}}} \tag{3} $$</p>
<p>而该函数对应的几何图如下：</p>
<p><img src="http://pic.l2h.site/perceptron-func-1.png" alt></p>
<p>Fig 7. Sigmoid函数的图形</p>
<p>从图上可以看出，因为输出值为平滑的0到1曲线，不会出现因为 $$ \sum_j w_j x_j +b\ $$ 的变化出现明显的跳变（输入的较小改变，得到的也是最终输出结果较小的变化），是较为理想的感知器函数。其数学公式如下：</p>
<p>$$ \begin{eqnarray} \Delta \mbox{output} \approx \sum_j \frac{\partial \, \mbox{output}}{\partial w_j} \Delta w_j + \frac{\partial \, \mbox{output}}{\partial b} \Delta b, \tag{5} \end{eqnarray} $$</p>
<p>即，使用Sigmoid函数的导数是一个线性的函数，随着输入参数的变化，output的值是线性增长的。</p>
<blockquote>
<p>有同学要问了，神经网络要解决的是一个是非题，但Sigmoid函数并非如此怎么办？</p>
<p>可以以0.5作为分界线，当Sigmoid的结果为0.5以上时，即认为最终的决策结果为“是”，反之为“否”</p>
</blockquote>
<h2 id="识别手写数字实战"><a href="#识别手写数字实战" class="headerlink" title="识别手写数字实战"></a>识别手写数字实战</h2><p>带着前面神经网络的基础知识，再回来看开篇提到的课题：识别手写数字。再看下边这张图：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/digits.png" alt></p>
<p>首先将问题分解：</p>
<ol>
<li>将上述图片分割为6个独立的数字</li>
<li>对6个独立的数字分别进行识别</li>
</ol>
<p><img src="http://neuralnetworksanddeeplearning.com/images/digits_separate.png" alt></p>
<p>图片分解</p>
<p>问题一像是如何识别图片边界的问题，对计算机来讲也比较复杂。不过本文暂时先考虑如何编程来解决问题2。</p>
<p>为了解决这个问题，定义一个三层神经网络：输入层包含784个神经元；中间隐层包含15个神经元；而输出层则为10个神经元分别代表数字0~9。</p>
<ul>
<li>输入层$$784=28*28$$，表示每个被分割出来的数字有28乘28个像素点组成。每个像素点值表示颜色深度：0值表示该像素位置为白色，1则表示黑色，0到1的中间数字表示介于黑白之间像素点的灰黑色深度。</li>
<li>隐层神经元个数的选择对学习效率是有影响的（事实上，如何选择这些初始值也是非常值得研究的课题）。本文从15作为初始选择，后边实例将会观察不同隐层神经元个数对学习结果的影响。</li>
<li>输出层则比较直觉，10个输出代表我们要识别的数字的10中可能。我们当然可以采用其他个数的神经元组合。例如：神经元代表每个二进制位的值，这样只需要4个神经元就能表示0<del>16的值。但事实上，这样的选择效果往往不会非常的理想。试想，1</del>10这10个数字的图片形状各不相同，如何用八个形状（4个bit*每个bit 2个值）来归纳这十个数字？</li>
</ul>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz12.png" alt></p>
<p>当然，本文采用的选择仅仅为一个启发而并非强制性的。或许有更好的参数，能进行更好更快的识别。</p>
<h3 id="梯度下降法学习"><a href="#梯度下降法学习" class="headerlink" title="梯度下降法学习"></a>梯度下降法学习</h3><p>前一节我们简单建立了用于学习的神经网络模型，接着需要有用于学习的数据集。<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST</a>收集了成千上万人的手写数字扫描数据，包含60000个训练数据以及10000个测试数据，均已被大小归一化乘28*28像素大小且居中对其。数据文件包括：</p>
<ul>
<li>train-images-idx3-ubyte: 训练集图片 </li>
<li>train-labels-idx1-ubyte: 训练集标签 </li>
<li>t10k-images-idx3-ubyte:  测试集图片 </li>
<li>t10k-labels-idx1-ubyte:  测试集标签</li>
</ul>
<p>图片文件格式如下：</p>
<p><strong>偏移</strong></p>
<p><strong>类型</strong></p>
<p><strong>数值</strong></p>
<p><strong>描述</strong></p>
<p>0000</p>
<p>32位整形</p>
<p>0x00000803(2051</p>
<p>MAGIC Number（魔数）</p>
<p>0004</p>
<p>32位整形</p>
<p>60000 或 10000</p>
<p>图片个数</p>
<p>0008</p>
<p>32位整形</p>
<p>28</p>
<p>像素行数</p>
<p>0012    </p>
<p>32位整形</p>
<p>28</p>
<p>像素列数</p>
<p>0016 </p>
<p>无符号字节</p>
<p>？</p>
<p>像素点</p>
<p>0017   </p>
<p>无符号字节</p>
<p>？</p>
<p>像素点</p>
<p>…….. </p>
<p>无符号字节</p>
<p>？</p>
<p>像素点</p>
<p>xxxx  </p>
<p>无符号字节</p>
<p>？</p>
<p>像素点</p>
<p>标签文件格式如下：</p>
<p><strong>偏移</strong></p>
<p><strong>类型</strong></p>
<p><strong>数值</strong></p>
<p>描述</p>
<p>0000</p>
<p>32位整形</p>
<p>0x00000801(2049</p>
<p>MAGIC Number（魔数）</p>
<p>0004</p>
<p>32位整形</p>
<p>60000</p>
<p>图片个数</p>
<p>0008</p>
<p>无符号字节</p>
<p>？</p>
<p>标签值（0-9）</p>
<p>0009</p>
<p>无符号字节</p>
<p>？</p>
<p>标签值（0-9）</p>
<p>…….. </p>
<p>无符号字节</p>
<p>？</p>
<p>标签值（0-9）</p>
<p>xxxx  </p>
<p>无符号字节</p>
<p>？</p>
<p>标签值（0-9）</p>
<p>本文使用$$x$$代表输入数据，它是一个28*28维向量。使用$$y=y(x)$$代表输出，其中$$y$$为10维向量。例，若输入图形$$x$$的输出结果为6，则用$$y(x)=(0,0,0,0,0,1,0,0,0,0)^T$$表示。</p>
<p>我们的目标就是找到所有权值及阈值，使得输出结果与所有的训练数据接近。我们可以定义如下误差函数：</p>
<p>$$\begin{eqnarray} C(w,b) \equiv \frac{1}{2n} \sum_x \| y(x) - a\|^2 \nonumber\end{eqnarray}$$</p>
<p>$$w$$表示网络中的所有权重，$$b$$代表所有的阈值，$$n$$表示训练数据总量，$$a$$则是$$x$$作为训练数据时的输出结果向量。那么该误差函数$$C(w,b)$$表示的就是目标函数输出与实际训练数据结果的均方误差，<em>mean squared error</em> or just(<em>MSE</em>)。当目标函数与输出结果接近时，$$C(w,b) \approx 0$$。因此我们的目标便是对所有训练数据，使得目标函数结果与训练数据的输出接近（当然最好是相等，XD）。接下来就该梯度下降算法出场了。</p>
<p>介绍梯度下降算法前，先思考一个问题：为什么我们的误差函数是均方误差，而不使用训练数据正确的个数？会这样考虑的原因是：用数量作为误差函数，并非平滑函数。即，对权值或者阈值微小修改并不会对正确数量产生大的变化。这样的话很难对权重或者阈值修正来得到机器学习效果的提升。而如果我们采用较为平滑的误差函数，则能通过对权重或者阈值的变化得到提升。</p>
<p>这里又有疑问了：虽然是要用较平滑的函数作为误差函数，为什么一定要是均方误差函数？当然不必，之后文章会再研究是否有其他选择。但是对了解神经网络基础来讲，这个均方误差函数便已足够。</p>
<p>如前所述，我们训练的目标，是求出可以使得误差函数最小的所有的权值和阈值，而我们将会使用梯度下降算法。为了对这一算法进行阐述，我们先简化一下我们的问题：另我们的输入只有两个像素点，对应两个权值$$v_1和v_2$$，接着我们可以对应扩展到多个权值或阈值的场景。</p>
<p>如前所述，我们训练的目标，是求出可以使得误差函数最小的所有的权值和阈值，而我们将会使用梯度下降算法。为了对这一算法进行阐述，我们先简化一下我们的问题：另我们的输入只有两个像素点，对应两个权值$$v_1和v_2$$，接着我们可以对应扩展到多个权值或阈值的场景。</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/valley.png" alt></p>
<p>全局最小</p>
<p>对上图来讲，我们可以一眼大概看出C的全局最小的值，得到对应的$$v_1和v_2$$。但是事实上，C往往是一个复杂的函数包含了许多参数，也不可能一眼就看出其最小值。而解决此类问题（求C最小值）的一种分析方法就是微积分: 我们可以通过求导来找到使C达到最小的点。当C函数参数较少的时候，或许可以幸运地的找到。但是当参数更多时，这种方法便成了噩梦。不幸的是，往往我们的神经网络中有多得多的参数–最大的神经网络可能会有成千上万个权重和阈值要求出，是不可能用微积分得到最小值的。</p>
<p>不过不要担心，幸运的是我们可以用其他算法来做这件事并得到很好的效果。首先我们把误差函数的看作一个山谷（就如上图一样），想象有一个球从斜坡滚向山谷。根据日常经验，这个球最终会滚向谷底。那么或许我们可以用这种方法来找到函数的最小值？可以随机选择一个起始点，并模拟球向山谷滚动的运动。我们可以通过对C求导（或者有时是二次导数）来做这样的模拟，并得到“山谷的形状”，最终得到球该如何向下滚动。</p>
<p>请放心，我们这里不是讨论像牛顿定律这种物理。为了更准确地描述这个问题，假设这个球向$$v_1$$和 $$v_2$$ 放心分别移$$ \Delta v_1 $$ 和 $$ \Delta v_2$$ 。通过微积分我们可以得到：</p>
<p>$$\begin{eqnarray} \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 + \frac{\partial C}{\partial v_2} \Delta v_2. \tag{6}\end{eqnarray}$$</p>
<p>我们需要找到一种方式选择 $$\Delta v_1 和\Delta v_2 这样\Delta C $$是负值，即我们的球可以向谷底移动。我们首先定义向量 $$ \Delta v \equiv (\Delta v_1, \Delta v_2)^T $$ 来表示每个维度上的变化，其中T为矩阵转置。而C对每个变量偏导数组成向量，如下：</p>
<p>$$\begin{eqnarray} \nabla C \equiv \left( \frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2} \right)^T. \tag{7}\end{eqnarray} $$</p>
<p>其中 $$ \nabla $$ 符号表示梯度向量。公式(6)和(7)合并：</p>
<p>$$ \begin{eqnarray} \Delta C \approx \nabla C \cdot \Delta v. \tag{8}\end{eqnarray} $$</p>
<p>通过以上公式，我们可以得到一种必定可以另 $$ \Delta C $$ 为负值的方法。选择</p>
<p>$$ \begin{eqnarray} \Delta v = -\eta \nabla C, \tag{9}\end{eqnarray} $$</p>
<p>其中 $$ -\eta$$为一个比较小的正值，而根据公式(9)即知道 $$ \Delta C $$ 一定为负值或0。看起来我就可以用公式(9)来更新我们的参数，来保证我们的小球一定是向谷底移动的。即每次对参数做 $$ \Delta v = -\eta \nabla C $$ 大小的修正，最后得到全局最小（即我们的球也滚到了底部）。这样解释了算法的名称“梯度下降”。</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/valley_with_ball.png" alt></p>
<p>简要说明梯度下降的思想后，我们如何将它用在神经网络中？注意到$$ v_1 和v_2$$ 是我们为了描述问题简化的，它们代表权值 $$ w_x 或者阈值 b$$ ，那么对公式(9)提到的 $$ v_1 和v_2$$ 更新方法，也同样适用在$$ w_x 或 b$$ ，即得到公式：</p>
<p>$$ \begin{eqnarray} w_k &amp; \rightarrow &amp; w_k’ = w_k-\eta \frac{\partial C}{\partial w_k} \tag{10}\\ b_l &amp; \rightarrow &amp; b_l’ = b_l-\eta \frac{\partial C}{\partial b_l}. \tag{11}\end{eqnarray} $$</p>
<p>对前文误差函数求偏导得到， $$ \nabla C = \frac{1}{n} \sum_x \nabla C_x $$，注意这里是对所有的训练数据做求偏导动作，当训练数据量很大时，学习时间会变得非常长。这时一种解决方案是，每次学习只取训练数据集的一部分m个，只要保证最后所有训练数据都有被取到且平均即可，那么公式为，<br>$$ \nabla C = \frac{1}{m} \sum_x \nabla C_x $$ 。最后我们得到参数的更新公式：</p>
<p>$$ \begin{eqnarray} w_k &amp; \rightarrow &amp; w_k’ = w_k-\frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial w_k} \tag{12}\\ b_l &amp; \rightarrow &amp; b_l’ = b_l-\frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial b_l}, \tag{13}\end{eqnarray} $$</p>
<h2 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h2><p>介绍了这么多理论，还是直接看看代码吧。本文参考的<a href="http://neuralnetworksanddeeplearning.com/index.html" target="_blank" rel="noopener">原文</a>使用<a href="http://www.numpy.org/" target="_blank" rel="noopener">numpy</a>作为机器学习库。代码放在<a href="https://github.com/lambertdev/nn-and-dl" target="_blank" rel="noopener">Github</a>。</p>
<p>代码如下：</p>
<p>#### Libraries<br># 标准随机库<br>import random</p>
<p># numpy库<br>import numpy as np</p>
<p>class Network(object):</p>
<pre><code>def __init__(self, sizes):
    &quot;&quot;“
    sizes定义是每层神经网络的神经元（感知器）个数。例如若sizes=[2,3,1]，那么它代表输入层2个感知器，隐层3个感知器，最后一层1个感知器。
    &quot;&quot;&quot;
    self.num_layers = len(sizes)
    self.sizes = sizes
    #初始化每层的权值w和b（bias即负的阈值）
    self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
    self.weights = [np.random.randn(y, x)
                    for x, y in zip(sizes[:-1], sizes[1:])]

def feedforward(self, a):
    &quot;&quot;&quot;返回感知器的输出z&quot;&quot;&quot;
    for b, w in zip(self.biases, self.weights):
        a = sigmoid(np.dot(w, a)+b)
    return a

def SGD(self, training_data, epochs, mini_batch_size, eta,
        test_data=None):
    &quot;&quot;&quot;使用前文提到的‘每次学习只取训练数据集的一部分m个，只要保证最后所有训练数据都有被取到且平均即可’算法进行训练&quot;&quot;&quot;
    if test_data: n_test = len(test_data)
    n = len(training_data)
    #训练轮数
    for j in xrange(epochs):
        #随机打乱训练数据
        random.shuffle(training_data)
        #根据训练最小包的个数分出多个训练包
        mini_batches = [
            training_data[k:k+mini_batch_size]
            for k in xrange(0, n, mini_batch_size)]
        #对每个训练数据包进行训练，权值w和b进行更新
        for mini_batch in mini_batches:
            self.update_mini_batch(mini_batch, eta)
        #用测试数据对训练结果进行计算准确率
        if test_data:
            print &quot;Epoch {0}: {1} / {2}&quot;.format(
                j, self.evaluate(test_data), n_test)
        else:
            print &quot;Epoch {0} complete&quot;.format(j)

def update_mini_batch(self, mini_batch, eta):
    &quot;&quot;&quot;使用后传播方法对训练数据包进行训练（更新w和b）&quot;&quot;&quot;
    #初始化所有的b和w
    nabla_b = [np.zeros(b.shape) for b in self.biases]
    nabla_w = [np.zeros(w.shape) for w in self.weights]
    #根据包中所有的训练数据计算w和b要变化的幅度nabla_b和nabla_w
    for x, y in mini_batch:
        delta_nabla_b, delta_nabla_w = self.backprop(x, y)
        nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
        nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
    #根据计算的更新幅度更新w和b
    self.weights = [w-(eta/len(mini_batch))*nw
                    for w, nw in zip(self.weights, nabla_w)]
    self.biases = [b-(eta/len(mini_batch))*nb
                   for b, nb in zip(self.biases, nabla_b)]

def backprop(self, x, y):
    nabla_b = [np.zeros(b.shape) for b in self.biases]
    nabla_w = [np.zeros(w.shape) for w in self.weights]
    # feedforward
    activation = x
    activations = [x] # list to store all the activations, layer by layer
    zs = [] 
    # 根据w和b计算训练数据输入得到每个神经元输出
    for b, w in zip(self.biases, self.weights):
        z = np.dot(w, activation)+b
        zs.append(z)
        activation = sigmoid(z)
        activations.append(activation)
    # 计算输出层对w和b的偏导
    delta = self.cost_derivative(activations[-1], y) * \
        sigmoid_prime(zs[-1])
    nabla_b[-1] = delta
    nabla_w[-1] = np.dot(delta, activations[-2].transpose())
    # 从倒数第二层到输入层计算每层w和b的更新值，这里的为BP神经网络算法的实现方法，后续会再介绍其原理。
    for l in xrange(2, self.num_layers):
        z = zs[-l]
        sp = sigmoid_prime(z）
        delta = np.dot(self.weights[-l+1].transpose(), delta) * sp
        nabla_b[-l] = delta
        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())
    return (nabla_b, nabla_w)

def evaluate(self, test_data):
    #此处比较容易理解，使用测试数据和我们计算得到的w和b代入的公式得到结果对比计算当前学习的准确率
    test_results = [(np.argmax(self.feedforward(x)), y)
                    for (x, y) in test_data]
    return sum(int(x == y) for (x, y) in test_results)

def cost_derivative(self, output_activations, y):
    return (output_activations-y)</code></pre><p>#sigmoid函数<br>def sigmoid(z):<br>    “””The sigmoid function.”””<br>    return 1.0/(1.0+np.exp(-z))<br>#sigmoid函数的倒数<br>def sigmoid_prime(z):<br>    “””Derivative of the sigmoid function.”””<br>    return sigmoid(z)*(1-sigmoid(z))</p>
<h2 id="待续"><a href="#待续" class="headerlink" title="待续"></a>待续</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol>
<li>《<a href="http://neuralnetworksanddeeplearning.com/index.html" target="_blank" rel="noopener">Neural Networks and Deep Learning</a>》</li>
</ol>

    </div>

    
    
    
      
  <div class="popular-posts-header">Related Posts</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="/2019/07/10/e5-8d-b7-e7-a7-af-e7-a5-9e-e7-bb-8f-e7-bd-91-e7-bb-9ccnn-e7-9a-84-e7-90-86-e8-a7-a3/" rel="bookmark">卷积神经网络(CNN)的理解</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="/2019/07/14/e6-9c-ba-e5-99-a8-e5-ad-a6-e4-b9-a0-e5-87-a0-e4-b8-aa-e8-a6-81-e7-b4-a0/" rel="bookmark">机器学习几个要素</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="/2019/02/05/machine-learning-neural-network-2/" rel="bookmark">神经网络和深度学习(2) -- 后向传播算法原理</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="/2019/07/09/e6-9c-ba-e5-99-a8-e5-ad-a6-e4-b9-a0-e4-b8-ba-e4-bb-80-e4-b9-88-e4-bd-bf-e7-94-a8-e7-8b-ac-e7-83-adone-hot-encoding-e7-bc-96-e7-a0-81/" rel="bookmark">机器学习为什么使用独热(One-hot Encoding)编码</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="/2018/12/02/mac-os-install-pytorch/" rel="bookmark">MAC OS本地安装PyTorch</a></div>
      
    </li>
  
  </ul>

        
      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
            
              <a href="/tags/神经网络/" rel="tag"># 神经网络</a>
            
              <a href="/tags/Neural-Network/" rel="tag"># Neural Network</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2019/01/29/linux-memory-init-repost/" rel="next" title="Linux內存初始化过程(ZZ)">
                  <i class="fa fa-chevron-left"></i> Linux內存初始化过程(ZZ)
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2019/02/05/machine-learning-neural-network-2/" rel="prev" title="神经网络和深度学习(2) -- 后向传播算法原理">
                  神经网络和深度学习(2) -- 后向传播算法原理 <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    
  <div class="comments" id="comments">
    <div id="SOHUCS"></div>
  </div>
  
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#概念"><span class="nav-number">2.</span> <span class="nav-text">概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#感知器"><span class="nav-number">2.1.</span> <span class="nav-text">感知器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络"><span class="nav-number">2.2.</span> <span class="nav-text">神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sigmoid神经元"><span class="nav-number">2.3.</span> <span class="nav-text">Sigmoid神经元</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#识别手写数字实战"><span class="nav-number">3.</span> <span class="nav-text">识别手写数字实战</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降法学习"><span class="nav-number">3.1.</span> <span class="nav-text">梯度下降法学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#代码实例"><span class="nav-number">4.</span> <span class="nav-text">代码实例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#待续"><span class="nav-number">5.</span> <span class="nav-text">待续</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献"><span class="nav-number">6.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.jpg"
      alt="Lambert">
  <p class="site-author-name" itemprop="name">Lambert</p>
  <div class="site-description" itemprop="description">旅行日记，Linux技术，机器学习，建站技巧</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">129</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">113</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/lambertdev" title="GitHub &rarr; https://github.com/lambertdev" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:lambertdev@gmail.com" title="E-Mail &rarr; mailto:lambertdev@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      友情链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.echoteen.com/" title="https://www.echoteen.com/" rel="noopener" target="_blank">Echo少年</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://mlldxe.cn/" title="http://mlldxe.cn/" rel="noopener" target="_blank">Mlldxe’s Blog</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://oldpan.me/" title="https://oldpan.me/" rel="noopener" target="_blank">Oldpan的个人博客</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://www.moerats.com/" title="https://www.moerats.com/" rel="noopener" target="_blank">Rat's Blog</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://www.supert.net.cn/" title="http://www.supert.net.cn/" rel="noopener" target="_blank">七年。</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://www.gazyip.cn/" title="https://www.gazyip.cn/" rel="noopener" target="_blank">八个比特</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://www.sharexbar.com/" title="http://www.sharexbar.com/" rel="noopener" target="_blank">分享巴中</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://blog.isoyu.com/" title="https://blog.isoyu.com/" rel="noopener" target="_blank">姬长信</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://masuit.com/" title="https://masuit.com/" rel="noopener" target="_blank">懒得勤快博客</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://www.zahuiw.com/" title="http://www.zahuiw.com/" rel="noopener" target="_blank">杂烩网</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://www.yangqq.com/" title="http://www.yangqq.com/" rel="noopener" target="_blank">杨青-"青姐"</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://www.yanshisan.cn/" title="https://www.yanshisan.cn/" rel="noopener" target="_blank">燕十三</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://www.tianshan277.com/" title="http://www.tianshan277.com/" rel="noopener" target="_blank">田珊珊个人博客</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://www.52ecy.cn/" title="https://www.52ecy.cn/" rel="noopener" target="_blank">阿珏博客</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://asciiflow.com/" title="http://asciiflow.com/" rel="noopener" target="_blank">在线ASCii绘图</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://daohang.lusongsong.com/" title="http://daohang.lusongsong.com/" rel="noopener" target="_blank">博客大全</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://travel-hare.l2h.site/" title="http://travel-hare.l2h.site/" rel="noopener" target="_blank">旅行的兔撸撸</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://neuralnetworksanddeeplearning.com/" title="http://neuralnetworksanddeeplearning.com/" rel="noopener" target="_blank">神经网络深度学习</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://pin.l2h.site" title="https://pin.l2h.site" rel="noopener" target="_blank">惠拼购</a>
        </li>
      
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright"><a href="http://www.beian.miit.gov.cn" rel="noopener" target="_blank">蜀ICP备17001636号-1 </a>
  <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=" rel="noopener" target="_blank"> </a>&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lambert</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a></div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a></div>

        












        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.0"></script><script src="/js/motion.js?v=7.4.0"></script>
<script src="/js/schemes/muse.js?v=7.4.0"></script>
<script src="/js/next-boot.js?v=7.4.0"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  

  <script>
  (function() {
    var appid = 'cyutixI06';
    var conf = 'prod_3e046acc8875533ad4449ee8d8e0fb14';
    var width = window.innerWidth || document.documentElement.clientWidth;
    if (width < 960) {
      window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>');
    } else {
      var loadJs=function(d,a){var c=document.getElementsByTagName("head")[0]||document.head||document.documentElement;var b=document.createElement("script");b.setAttribute("type","text/javascript");b.setAttribute("charset","UTF-8");b.setAttribute("src",d);if(typeof a==="function"){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e==="loaded"||e==="complete"){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs("https://changyan.sohu.com/upload/changyan.js",function(){window.changyan.api.config({appid:appid,conf:conf})});
    }
  })();
  </script>
  <script src="https://assets.changyan.sohu.com/upload/plugins/plugins.count.js"></script>

</body>
</html>
