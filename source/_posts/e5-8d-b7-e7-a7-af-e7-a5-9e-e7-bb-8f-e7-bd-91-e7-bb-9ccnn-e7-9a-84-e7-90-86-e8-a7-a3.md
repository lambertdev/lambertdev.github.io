---
title: 卷积神经网络(CNN)的理解
tags:
  - CNN
  - 机器学习
  - 神经网络
url: 2343.html
id: 2343
categories:
  - 机器学习
date: 2019-07-10 15:54:26
---

机器学习的目标是可以让计算机像人一样对事物进行理解，近些年深度学习得到了深入的发展，并有了广泛地产业化应用。一些常见的机器学习应用包括音视频识别、音视频分类、自然语言处理（NPL）等。

卷积神经网络（**Convolutional Neural Network，CNN**）最早提出于1979年，于今时在机器学习领域得到了广泛地应用，特别是在执行图片分类、识别等任务时，有着绝佳的效果。这是因为其在处理大量数据集时，计算复杂度上有着很大的优势。

框架
--

![](https://l2h.site/wp-content/uploads/2019/07/1.jpg)

神经网络的框架如图所示，它主要由以下几个部分组成：

*   卷积
*   (ReLU)非线性化
*   池化
*   分类

以上为卷积神经网络必不可少的部分，接下来我们试着对以上步骤分别进行理解（本文不涉及数学公式部分）。

CNN深入理解
-------

在对以上步骤进行介绍之前，我们先看一下计算机对视觉图像是如何进行理解的。如下图，对计算机来讲，每个图像都是以像素值作为元素的一组向量。每个像素点有RGB三种不同的**_通道_**，每个通道的取值为0-255的数字。而**灰度**图像，往往是只有一个通道的图像。为介绍的简便性，本文考虑只有一个通道的灰度图像。

### 卷积

CNN的名称即来自于“卷积”这一步骤。后者的主要目标是从输入中提取出可以用于分类的特征，同时保留像素间的空间距离。

考虑如下图左5\*5像素大小的灰度图像，其中像素值为0或1（注意，这只是为了说明问题的一个特殊例子。灰度图像的像素点取值范围仍是0-255），以及图右的3\*3矩阵

*   ![](https://l2h.site/wp-content/uploads/2019/07/2-2.png)
    
    图像像素点
    
*   ![](https://l2h.site/wp-content/uploads/2019/07/3.png)
    
    过滤矩阵
    

卷积操作即是类似如下动图对5\*5图像和3\*3矩阵的操作。橘色的矩阵在绿色原始图像像素矩阵上进行1像素大小滑动（Stride），每次对叠加元素间的积求和到新的向量矩阵元素（下图粉色矩阵）。

![](https://l2h.site/wp-content/uploads/2019/07/4.gif)

在卷积神经网络概念中，橘色的3*3矩阵被成为“过滤器”或“核函数”，而滑动过滤器并计算粉色向量的过程成为“特征卷积”或者“特征映射”。即，我们使用橘色的矩阵作为了原始图像的特征映射器。

很明显，从动图可以看出，对同一个图像不同的过滤矩阵值会产品不同的特征映射矩阵。考虑下图输入图像，我们可以从下表中看出，不同的过滤矩阵，如何影响到卷积的效果：

![](https://l2h.site/wp-content/uploads/2019/07/5.png)

  

*   ![](https://l2h.site/wp-content/uploads/2019/07/屏幕快照-2019-07-10-下午2.21.42.png)
    
    不同卷积操作对原始图的影响
    

从下图也可以很清楚地看到卷积操作的过程及产生的结果（两个不同的过滤矩阵，过滤出了同一图像的不同特征矩阵。）：

![](https://wx2.sinaimg.cn/mw690/62d92af0ly1g4uq9zl5isg20dc07k7wm.gif)

在实际应用，CNN的训练过程会对过滤矩阵中的参数值进行学习（当然在这之前“我们”要负责选择过滤矩阵的数量、矩阵的大小以及网络的结构）。我们选择越多的过滤矩阵，卷积过程便得到越多的图像特征，这样网络识别图像的效果也越好。

卷积层的大小主要由以下三个参数来决定（学习开始前“我们”需要决定这些参数）：

*   深度(Depth): 即卷积矩阵的数量，例如上图狗狗相片我们采取的Edge Detection、Box Blur矩阵等。不同的矩阵维度表示不同的特征。
*   步长(Stride)：卷积矩阵每次在原始图像像素矩阵上的滑动距离。例，2代表每次滑动2个像素的宽度
*   0填充(Zero-Padding)：有时需要在原始数据边界填充0作为被卷积的对象。此时称为宽卷积（Wide Convolution），反之称为窄卷积（Narrow Convolution）。0填充的一个作用是，当移动步长超过1时，有时移动会超出数据边界，此时适当做一些0填充来让移动可以顺利进行

### 非线性化(ReLU)

卷积操作之后，会对数据进行非线性化。ReLU为常用非线性化函数（Rectified Linear Uni），下图为该函数的坐标轴表示：

![](https://l2h.site/wp-content/uploads/2019/07/screen-shot-2016-08-10-at-2-23-48-am.png)

ReLU对卷积化后的输出矩阵元素的负值进行处理，替换成0，主要目标便是将数据非线性化（因为真实世界中的数据往往也是非线性化的，而卷积是一个线性过程，需要对卷积后的数据做非线性处理）。

下图描述了ReLU操作对卷积操作后的特征矩阵的影响：

![](https://l2h.site/wp-content/uploads/2019/07/screen-shot-2016-08-07-at-6-18-19-pm.png)

也有一些其他非线性函数如Sigmoid和tanh（最朴素的神经网络里也会用到这些函数），ReLU在实际使用中性能优于这两个函数。

### 池化过程(Polling)

空间池化（也成为“子抽样”）可以对卷积和非线性化的特征矩阵做有效降维，同时保持了矩阵的重要信息。池化主要用到的函数包括最大值(Max)、平均值(Average)、求和(Sum)等。

以最大值池化为例，我们在特征矩阵上定义空间近邻（如下图，2*2的窗口），取出其中每个窗口的最大元素，重新组成特征矩阵。若该层卷积选择了多个过滤窗口，那么我们对每个卷积化特征矩阵做池化处理得到池化后的特征矩阵。

![](https://l2h.site/wp-content/uploads/2019/07/screen-shot-2016-08-10-at-3-38-39-am.png)

下图显示了图片被池化后的效果：

![](https://l2h.site/wp-content/uploads/2019/07/screen-shot-2016-08-07-at-6-11-53-pm.png)

池化的主要作用就是有效降低输入数据的空间大小，节省计算量。其优势总结如下：

*   不损失信息前提下，减小特征数据
*   减少神经网络需要计算的参数数量和计算量，因此可以有效抑制过拟合
*   减少输入数据因为转化变形等操作带来的影响

以上过程介绍了构建CNN的基础成分卷积、非线性化和池化。这些过程的最终目的是得到与图片相关的、小数据量的、且能有效表达图像特征的特征矩阵。而这个特征矩阵的目的则是为最后一步，全连接神经网络的计算做准备，即我们提到的“分类”步骤

全连接神经网络的主要原理可以参考本站<[神经网络和深度学习](https://l2h.site/2019/02/02/machine-learning-neural-network-1/)>一文的介绍，此处不做过多阐述。  

### 总结

总结使用卷积神经网络进行机器学习的主要过程为：

1.  选择卷积神经网络的卷积、非线性化、池化等参数。同时初始化各个过程权值的初始值（可以随机化处理）
2.  输入图像，并得到图片输出。
3.  计算图片输出和实际输出的损失。
4.  使用BP算法来优化神经网络，降低损失。

欢迎留言探讨